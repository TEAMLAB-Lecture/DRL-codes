{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-Step Actor-Critic for LunarLander-v2 (PyTorch & Gymnasium) - Granular Cells\n",
        "이 노트북은 Gymnasium의 LunarLander-v2 환경에서 N-Step Actor-Critic 알고리즘을 사용하여 강화학습 에이전트를 훈련합니다. 각 단계를 세분화된 셀로 나누어 Colab과 같은 환경에서 단계별 실행 및 이해를 돕도록 구성했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Installs and Imports\n",
        "- 필요한 라이브러리를 설치하고 임포트합니다.\n",
        "- Colab 환경에서는 `gymnasium[box2d]`와 `opencv-python-headless` 설치가 필요할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab 또는 유사 환경에서 실행 시 필요한 라이브러리 설치 (주석 해제 후 실행)\n",
        "# !pip install gymnasium[box2d] opencv-python-headless matplotlib seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 라이브러리 임포트 ---\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2  # OpenCV for video writing\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "from collections import deque\n",
        "import time # 시간 측정용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Logging Configuration\n",
        "- 훈련 과정을 기록하기 위한 로깅 시스템을 설정합니다.\n",
        "- 로그 파일과 비디오, 플롯을 저장할 고유한 디렉토리를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 로깅 설정 함수 정의 ---\n",
        "def setup_logging():\n",
        "    \"\"\"훈련 로그를 파일과 콘솔에 기록하고, 로그 디렉토리 경로를 반환합니다.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_dir = f\"lunar_lander_a2c_nstep_{timestamp}\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    print(f\"Log directory created: {log_dir}\")\n",
        "\n",
        "    # 로거 설정 (파일 핸들러 + 스트림 핸들러)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.INFO) # 로그 레벨 설정\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 파일 핸들러\n",
        "    file_handler = logging.FileHandler(os.path.join(log_dir, \"training.log\"))\n",
        "    file_handler.setFormatter(formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    # 스트림 핸들러 (콘솔 출력)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    logger.addHandler(stream_handler)\n",
        "\n",
        "    # 다른 라이브러리의 로깅 레벨 조절 (선택 사항)\n",
        "    logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "\n",
        "    return logger, log_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 로거 및 디렉토리 초기화 ---\n",
        "logger, log_base_dir = setup_logging()\n",
        "video_dir = os.path.join(log_base_dir, \"videos\")\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "logger.info(f\"Video save directory: {video_dir}\")\n",
        "plot_save_path = os.path.join(log_base_dir, \"rewards_history.png\")\n",
        "logger.info(f\"Plot save path: {plot_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Network Definitions\n",
        "- Actor-Critic 알고리즘에 사용될 신경망 모델을 정의합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Actor Network\n",
        "- 상태(State)를 입력받아 각 행동(Action)의 확률 분포를 출력합니다. (Policy Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor Network: Maps state to action probabilities.\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1) # Probabilities for each action\n",
        "        )\n",
        "        logger.debug(\"Actor network instance created.\")\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Critic Network\n",
        "- 상태(State)를 입력받아 해당 상태의 가치(Value)를 추정합니다. (Value Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic Network: Maps state to its estimated value.\"\"\"\n",
        "    def __init__(self, state_dim, hidden_dim=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1) # Single value output representing state value\n",
        "        )\n",
        "        logger.debug(\"Critic network instance created.\")\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. N-Step Actor-Critic Agent\n",
        "- 위에서 정의한 Actor, Critic 네트워크를 사용하여 N-Step Actor-Critic 에이전트를 구현합니다.\n",
        "- 이 클래스는 에이전트의 초기화, 행동 선택, 경험 저장, N-Step 리턴 계산, 네트워크 업데이트 로직을 포함합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NStepActorCritic:\n",
        "    \"\"\"N-Step Actor-Critic Agent implementation.\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, n_steps=5):\n",
        "        \"\"\"Initializes the agent, networks, optimizers, and buffers.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"Agent initializing on device: {self.device}\")\n",
        "\n",
        "        # Networks\n",
        "        self.actor = Actor(state_dim, action_dim).to(self.device)\n",
        "        self.critic = Critic(state_dim).to(self.device)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "\n",
        "        # Parameters\n",
        "        self.gamma = gamma\n",
        "        self.n_steps = n_steps\n",
        "        self.step_losses = [] # Store losses for averaging\n",
        "\n",
        "        # N-Step Buffers using deque for automatic old data removal\n",
        "        self.state_buffer = deque(maxlen=n_steps)\n",
        "        self.action_buffer = deque(maxlen=n_steps)\n",
        "        self.reward_buffer = deque(maxlen=n_steps)\n",
        "        self.next_state_buffer = deque(maxlen=n_steps)\n",
        "        self.done_buffer = deque(maxlen=n_steps)\n",
        "        self.prob_buffer = deque(maxlen=n_steps) # Stores action probabilities\n",
        "\n",
        "        logger.info(f\"NStepActorCritic initialized: n_steps={n_steps}, lr_actor={lr_actor}, lr_critic={lr_critic}, gamma={gamma}\")\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Selects an action based on the current policy (actor network).\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        self.actor.eval() # Set actor to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            probs = self.actor(state_tensor).squeeze(0) # Get action probabilities\n",
        "        self.actor.train() # Set actor back to training mode\n",
        "\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample() # Sample action from distribution\n",
        "        action_prob = probs[action] # Get probability of the chosen action\n",
        "        # logger.debug(f\"State: {state}, Probs: {probs.cpu().numpy()}, Action: {action.item()}, Prob: {action_prob.item():.4f}\")\n",
        "        return action.item(), action_prob # Return action index and its probability tensor\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done, action_prob):\n",
        "        \"\"\"Stores a transition in the N-step buffers.\"\"\"\n",
        "        self.state_buffer.append(state)\n",
        "        self.action_buffer.append(action)\n",
        "        self.reward_buffer.append(reward)\n",
        "        self.next_state_buffer.append(next_state)\n",
        "        self.done_buffer.append(done)\n",
        "        # Detach prob tensor from graph and move to CPU before storing\n",
        "        self.prob_buffer.append(action_prob.detach().cpu())\n",
        "\n",
        "    def compute_n_step_returns(self):\n",
        "        \"\"\"Calculates N-step returns based on rewards and the final state value.\"\"\"\n",
        "        if not self.reward_buffer: return [] # Return empty if buffer is empty\n",
        "\n",
        "        returns = deque(maxlen=self.n_steps)\n",
        "        last_state = self.next_state_buffer[-1]\n",
        "        last_done = self.done_buffer[-1]\n",
        "\n",
        "        # Estimate value of the state after N steps\n",
        "        if last_done:\n",
        "            R = 0.0\n",
        "        else:\n",
        "            self.critic.eval() # Set critic to evaluation mode\n",
        "            with torch.no_grad():\n",
        "                last_state_tensor = torch.FloatTensor(last_state).unsqueeze(0).to(self.device)\n",
        "                R = self.critic(last_state_tensor).item()\n",
        "            self.critic.train() # Set critic back to training mode\n",
        "\n",
        "        # Calculate returns backward from the Nth step\n",
        "        for i in reversed(range(len(self.reward_buffer))):\n",
        "            R = self.reward_buffer[i] + self.gamma * R * (1 - self.done_buffer[i])\n",
        "            returns.appendleft(R) # Add to the left to maintain order\n",
        "\n",
        "        return list(returns)\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Performs a learning update for both Actor and Critic networks.\"\"\"\n",
        "        # Check if buffer has enough transitions\n",
        "        if len(self.state_buffer) < self.n_steps:\n",
        "            # logger.debug(\"Buffer not full enough for update.\")\n",
        "            return 0.0, 0.0 # Not ready to update\n",
        "\n",
        "        # Calculate N-step returns\n",
        "        n_step_returns = self.compute_n_step_returns()\n",
        "        if not n_step_returns: # Handle empty return case\n",
        "             return 0.0, 0.0\n",
        "\n",
        "        returns_tensor = torch.FloatTensor(n_step_returns).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Prepare tensors from buffers\n",
        "        states_tensor = torch.FloatTensor(np.array(self.state_buffer)).to(self.device)\n",
        "        # Retrieve stored action probabilities (already detached)\n",
        "        # Need to stack them correctly and move to device\n",
        "        action_probs_tensor = torch.stack(list(self.prob_buffer)).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # --- Critic Update ---\n",
        "        current_values = self.critic(states_tensor) # V(s_t)\n",
        "        advantages = returns_tensor - current_values  # TD Error: R_t^(n) - V(s_t)\n",
        "        critic_loss = advantages.pow(2).mean() # MSE Loss\n",
        "\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5) # Optional: Gradient Clipping\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        # --- Actor Update ---\n",
        "        log_probs = torch.log(action_probs_tensor + 1e-6) # Add epsilon for numerical stability\n",
        "        # Actor loss using policy gradient theorem (log_prob * advantage)\n",
        "        # detach advantages so its gradient doesn't affect actor loss calculation\n",
        "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
        "\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5) # Optional: Gradient Clipping\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # Store step loss for monitoring\n",
        "        total_step_loss = critic_loss.item() + actor_loss.item()\n",
        "        self.step_losses.append(total_step_loss)\n",
        "        # logger.debug(f\"Update successful. Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
        "\n",
        "        # Clear buffers for the next set of N-steps (deque manages this automatically)\n",
        "        # No explicit clear needed unless logic changes\n",
        "\n",
        "        return critic_loss.item(), actor_loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Helper Function: Video Saving\n",
        "- 에피소드의 프레임들을 받아 MP4 비디오 파일로 저장합니다. OpenCV 라이브러리를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_episode_video(frames, episode, video_dir):\n",
        "    \"\"\"Saves a list of frames as an MP4 video file.\"\"\"\n",
        "    if not frames:\n",
        "        logger.warning(f\"Attempted to save video for episode {episode}, but no frames were provided.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Ensure frame is NumPy array\n",
        "        if isinstance(frames[0], torch.Tensor):\n",
        "            frames = [f.cpu().numpy() for f in frames]\n",
        "\n",
        "        # Get frame dimensions\n",
        "        height, width, channels = frames[0].shape\n",
        "        if channels != 3:\n",
        "             logger.error(f\"Invalid frame shape for video saving: {frames[0].shape}. Expected 3 channels.\")\n",
        "             return\n",
        "\n",
        "        # Define video path and codec\n",
        "        video_path = os.path.join(video_dir, f\"episode_{episode:04d}.mp4\")\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # MP4 codec\n",
        "        fps = 30.0 # Frames per second\n",
        "\n",
        "        # Create VideoWriter object\n",
        "        video = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
        "        if not video.isOpened():\n",
        "            logger.error(f\"Failed to open video writer for path: {video_path}\")\n",
        "            return\n",
        "\n",
        "        logger.debug(f\"Saving video: {video_path}, Resolution: {width}x{height}, FPS: {fps}\")\n",
        "        # Write frames to video file\n",
        "        for frame in frames:\n",
        "            # Gymnasium's rgb_array is typically RGB, OpenCV expects BGR\n",
        "            video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        video.release() # Release the video writer\n",
        "        logger.debug(f\"Video saved successfully for episode {episode}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during video saving for episode {episode}: {e}\", exc_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Main Training Loop Function\n",
        "- 환경 설정, 에이전트 생성, 훈련 루프 실행 로직을 포함합니다.\n",
        "- 각 에피소드에서 환경과 상호작용하며 데이터를 수집하고 에이전트를 업데이트합니다.\n",
        "- 주기적으로 로그 출력, 비디오 저장, 보상 그래프 저장을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env_name=\"LunarLander-v2\", num_episodes=2000, n_steps=5, lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, save_video_interval=100, log_interval=10, plot_interval=50, random_seed=42):\n",
        "    \"\"\"Main function to train the N-Step Actor-Critic agent.\"\"\"\n",
        "    logger.info(f\"--- Starting Training ---\")\n",
        "    logger.info(f\"Environment: {env_name}, Episodes: {num_episodes}, N-Steps: {n_steps}\")\n",
        "    logger.info(f\"LR Actor: {lr_actor}, LR Critic: {lr_critic}, Gamma: {gamma}\")\n",
        "    logger.info(f\"Save Video Interval: {save_video_interval}, Log Interval: {log_interval}, Plot Interval: {plot_interval}\")\n",
        "    logger.info(f\"Random Seed: {random_seed}\")\n",
        "\n",
        "    try:\n",
        "        # Create environment with specified render mode\n",
        "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create environment '{env_name}': {e}\", exc_info=True)\n",
        "        return None, None, None # Return None if env creation fails\n",
        "\n",
        "    # Set seeds for reproducibility\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    # env.seed(random_seed) # Deprecated, use reset(seed=...) instead\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    logger.info(f\"State Dim: {state_dim}, Action Dim: {action_dim}\")\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = NStepActorCritic(state_dim, action_dim, n_steps=n_steps, lr_actor=lr_actor, lr_critic=lr_critic, gamma=gamma)\n",
        "\n",
        "    # History lists for plotting and analysis\n",
        "    rewards_history = []\n",
        "    avg_rewards_history = []\n",
        "    avg_losses_history = []\n",
        "    window_size = 100 # For calculating moving average reward\n",
        "\n",
        "    total_training_start_time = time.time()\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset(seed=random_seed + episode) # Reset env with new seed\n",
        "        episode_reward = 0\n",
        "        frames = []\n",
        "        steps_in_episode = 0\n",
        "        done = False\n",
        "        episode_losses = [] # Store losses for this specific episode\n",
        "\n",
        "        episode_start_time = time.time()\n",
        "\n",
        "        while not done:\n",
        "            # Render environment frame if needed for video saving\n",
        "            if episode % save_video_interval == 0:\n",
        "                try:\n",
        "                    frame = env.render()\n",
        "                    if frame is not None: frames.append(frame)\n",
        "                except Exception as render_err:\n",
        "                    logger.error(f\"Rendering error in ep {episode}: {render_err}\", exc_info=False)\n",
        "                    frames = [] # Avoid saving corrupted video\n",
        "                    break # Stop collecting frames for this episode\n",
        "\n",
        "            # Agent selects action\n",
        "            action, action_prob = agent.select_action(state)\n",
        "\n",
        "            # Environment steps forward\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Store experience\n",
        "            agent.store_transition(state, action, reward, next_state, done, action_prob)\n",
        "\n",
        "            # Perform agent update if buffer is ready or episode ends\n",
        "            if len(agent.state_buffer) >= n_steps or done:\n",
        "                c_loss, a_loss = agent.update()\n",
        "                if c_loss is not None and a_loss is not None: # Check if update happened\n",
        "                     episode_losses.append(c_loss + a_loss)\n",
        "\n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps_in_episode += 1\n",
        "\n",
        "            if done: break # Exit while loop if episode finished\n",
        "\n",
        "        # --- End of Episode ---\n",
        "        rewards_history.append(episode_reward)\n",
        "        avg_reward = np.mean(rewards_history[-window_size:]) if rewards_history else 0\n",
        "        avg_rewards_history.append(avg_reward)\n",
        "\n",
        "        # Calculate average loss for the episode\n",
        "        avg_episode_loss = np.mean(episode_losses) if episode_losses else 0\n",
        "        avg_losses_history.append(avg_episode_loss)\n",
        "\n",
        "        episode_end_time = time.time()\n",
        "        episode_duration = episode_end_time - episode_start_time\n",
        "\n",
        "        # Log progress periodically\n",
        "        if episode % log_interval == 0 or episode == num_episodes - 1:\n",
        "            logger.info(f\"Ep {episode:04d}/{num_episodes} | \"\n",
        "                        f\"Steps: {steps_in_episode:3d} | \"\n",
        "                        f\"Return: {episode_reward:7.2f} | \"\n",
        "                        f\"Avg Return ({window_size}ep): {avg_reward:7.2f} | \"\n",
        "                        f\"Avg Ep Loss: {avg_episode_loss:7.4f} | \"\n",
        "                        f\"Duration: {episode_duration:5.2f}s\")\n",
        "\n",
        "        # Save video periodically\n",
        "        if episode % save_video_interval == 0 and frames:\n",
        "            save_episode_video(frames, episode, video_dir)\n",
        "\n",
        "        # Save plot periodically\n",
        "        if episode % plot_interval == 0 or episode == num_episodes - 1:\n",
        "            try:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                # Plot rewards\n",
        "                plt.plot(rewards_history, label='Episode Reward', alpha=0.5, color='lightblue')\n",
        "                plt.plot(avg_rewards_history, label=f'Avg Reward (Last {window_size} ep)', linewidth=2, color='blue')\n",
        "                # Plot loss on secondary y-axis if desired (optional)\n",
        "                # ax2 = plt.gca().twinx()\n",
        "                # ax2.plot(avg_losses_history, label='Avg Episode Loss', color='orange', linestyle='--')\n",
        "                # ax2.set_ylabel('Average Loss', color='orange')\n",
        "                # ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "                plt.title(f\"Training Progress - {env_name} (N-Step A2C)\")\n",
        "                plt.xlabel(\"Episode\")\n",
        "                plt.ylabel(\"Total Reward\", color='blue')\n",
        "                plt.tick_params(axis='y', labelcolor='blue')\n",
        "                plt.legend(loc='upper left')\n",
        "                # if 'ax2' in locals(): plt.legend(loc='upper right') # Adjust legend location if secondary axis exists\n",
        "                plt.grid(True)\n",
        "                plt.savefig(plot_save_path)\n",
        "                plt.close() # Close plot to free memory\n",
        "                logger.debug(f\"Reward history plot updated and saved to {plot_save_path}\")\n",
        "            except Exception as plot_err:\n",
        "                logger.error(f\"Error saving plot: {plot_err}\", exc_info=True)\n",
        "\n",
        "    # --- End of Training ---\n",
        "    env.close() # Close the environment\n",
        "    total_training_end_time = time.time()\n",
        "    total_duration_seconds = total_training_end_time - total_training_start_time\n",
        "    logger.info(f\"--- Training completed in {total_duration_seconds:.2f} seconds ---\")\n",
        "\n",
        "    return rewards_history, avg_rewards_history, avg_losses_history # Return collected data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Execution\n",
        "- 이 섹션에서는 실제 훈련을 시작합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Set Training Hyperparameters\n",
        "- 훈련에 사용할 하이퍼파라미터를 설정합니다. 이 값을 변경하여 훈련 성능을 조절할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Training Hyperparameters ---\n",
        "ENV_NAME = \"LunarLander-v2\"\n",
        "NUM_EPISODES = 1500       # 훈련할 총 에피소드 수\n",
        "N_STEPS = 8              # N-스텝 리턴 계산 시 사용할 스텝 수\n",
        "LR_ACTOR = 5e-4          # Actor 학습률\n",
        "LR_CRITIC = 5e-4         # Critic 학습률\n",
        "GAMMA = 0.99             # 할인 계수 (Discount factor)\n",
        "SAVE_VIDEO_INTERVAL = 200 # 비디오 저장 주기 (에피소드 단위)\n",
        "LOG_INTERVAL = 10        # 로그 출력 주기 (에피소드 단위)\n",
        "PLOT_INTERVAL = 50       # 플롯 저장 주기 (에피소드 단위)\n",
        "RANDOM_SEED = 42         # 재현성을 위한 랜덤 시드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Run Training\n",
        "- 설정된 하이퍼파라미터로 `train` 함수를 호출하여 훈련을 시작하고 결과를 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Start Training ---\n",
        "logger.info(\"=\"*20 + \" Initiating Training Run \" + \"=\"*20)\n",
        "training_results = train(\n",
        "    env_name=ENV_NAME,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    n_steps=N_STEPS,\n",
        "    lr_actor=LR_ACTOR,\n",
        "    lr_critic=LR_CRITIC,\n",
        "    gamma=GAMMA,\n",
        "    save_video_interval=SAVE_VIDEO_INTERVAL,\n",
        "    log_interval=LOG_INTERVAL,\n",
        "    plot_interval=PLOT_INTERVAL,\n",
        "    random_seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Unpack results if training was successful\n",
        "if training_results:\n",
        "    rewards_hist, avg_rewards_hist, avg_losses_hist = training_results\n",
        "    logger.info(\"Training function executed successfully.\")\n",
        "else:\n",
        "    logger.error(\"Training function failed to execute properly.\")\n",
        "    rewards_hist, avg_rewards_hist, avg_losses_hist = None, None, None\n",
        "\n",
        "logger.info(\"=\"*20 + \" Training Run Finished \" + \"=\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Visualization\n",
        "- 훈련 과정에서 기록된 보상 및 손실 데이터를 사용하여 최종 그래프를 시각화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plot Final Results ---\n",
        "if rewards_hist and avg_rewards_hist and avg_losses_hist:\n",
        "    logger.info(\"Displaying final training results plots...\")\n",
        "    plt.figure(figsize=(18, 7))\n",
        "\n",
        "    # Plot Rewards\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards_hist, label='Episode Reward', alpha=0.6, color='lightblue')\n",
        "    plt.plot(avg_rewards_hist, label='Avg Reward (Smoothed)', linewidth=2.5, color='blue')\n",
        "    plt.title(\"Episode Rewards over Time\", fontsize=14)\n",
        "    plt.xlabel(\"Episode\", fontsize=12)\n",
        "    plt.ylabel(\"Total Reward\", fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Plot Losses\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(avg_losses_hist, label='Average Episode Loss', color='darkorange', linewidth=2)\n",
        "    plt.title(\"Average Training Loss per Episode\", fontsize=14)\n",
        "    plt.xlabel(\"Episode\", fontsize=12)\n",
        "    plt.ylabel(\"Average Loss\", fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.suptitle(f\"N-Step Actor-Critic Training Results ({ENV_NAME})\", fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show() # Display the plot in the notebook\n",
        "    logger.info(\"Final plots displayed.\")\n",
        "\n",
        "    # Save the final combined plot as well\n",
        "    try:\n",
        "        final_plot_path = os.path.join(log_base_dir, \"final_training_summary.png\")\n",
        "        plt.savefig(final_plot_path)\n",
        "        logger.info(f\"Final summary plot saved to {final_plot_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save final plot: {e}\", exc_info=True)\n",
        "    plt.close() # Close plot figure\n",
        "\n",
        "else:\n",
        "    logger.warning(\"No training results available to plot.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
