# Actor-Critic 알고리즘 구현

이 프로젝트는 Actor-Critic 알고리즘을 구현하여 CartPole 환경에서의 강화학습을 실습합니다.

## 주요 특징

- Actor와 Critic 네트워크의 동시 학습
- TD 오차 기반 Critic 업데이트
- Advantage 기반 Actor 업데이트
- 학습 과정의 시각화 및 기록

## 구현된 알고리즘

1. **Actor (정책 네트워크)**
   - 상태를 입력으로 받아 행동의 확률 분포를 출력
   - 3개의 완전 연결 레이어로 구성
   - ReLU 활성화 함수와 softmax 출력 사용

2. **Critic (가치 네트워크)**
   - 상태를 입력으로 받아 상태 가치를 출력
   - 3개의 완전 연결 레이어로 구성
   - ReLU 활성화 함수 사용

3. **학습 과정**
   - TD 오차 계산: δ_t = r_t + γV(s_{t+1}) - V(s_t)
   - Critic 업데이트: MSE 손실 함수 사용
   - Actor 업데이트: Advantage 기반 정책 그래디언트

## 실행 방법

1. Docker 컨테이너 빌드 및 실행:
```bash
./run.bat
```

2. 학습 과정 시각화:
- 학습 중에 생성되는 `frames_[timestamp]` 폴더에서 다음을 확인할 수 있습니다:
  - `training.mp4`: 학습 과정의 비디오
  - `rewards.png`: 에피소드별 보상 그래프

## 하이퍼파라미터

- Actor 학습률: 0.001
- Critic 학습률: 0.001
- 할인율(γ): 0.99
- 에피소드 수: 1000

## 참고사항

- CartPole-v1 환경에서 테스트되었습니다.
- 학습 과정은 실시간으로 시각화되며, 각 에피소드의 보상이 출력됩니다.
- 비디오 생성에는 OpenCV가 사용됩니다. 